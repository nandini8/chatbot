{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3696521069324384001\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3866165248\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 15544132128716349263\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/cornell movie-dialogs corpus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(convs):\n",
    "    conv = []\n",
    "    for text in convs:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"there's\", \"there is\", text)\n",
    "        text = re.sub(r\"how's\", \"how is\", text)\n",
    "        text = re.sub(r\"let's\", \"let us\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "        conv.append(text)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_length(convs):\n",
    "    final_convs = []\n",
    "    for text in convs:\n",
    "        if 2 <= len(text.split()) <= 30:\n",
    "            final_convs.append(text)\n",
    "            \n",
    "    return final_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_data():\n",
    "    id2line = {}\n",
    "    with open(path + 'movie_lines.txt', 'rb') as file:\n",
    "        for line in file:\n",
    "            list1 = str(line).split(\" +++$+++ \")\n",
    "            id2line[list1[0][2::]] = list1[-1][:-3:]\n",
    "\n",
    "    # print(id2line)\n",
    "\n",
    "\n",
    "    conversations = []\n",
    "    with open(path + 'movie_conversations.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            list1 = str(line).split(\" +++$+++ \")\n",
    "            list_of_conv = list1[-1].replace(\"'\", \"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\" \", \"\").split(\",\")\n",
    "            conversations.append([id2line[x] for x in list_of_conv])\n",
    "\n",
    "    #print(conversations)\n",
    "    encoders = []\n",
    "    decoders = []\n",
    "\n",
    "    for convs in conversations:\n",
    "        \n",
    "        clean_d = clean_data(convs)\n",
    "        conv = filter_length(clean_d)\n",
    "        \n",
    "        if len(conv) %2 != 0:\n",
    "            conv = conv[:-1]\n",
    "        for i in range(len(conv)):\n",
    "            if i%2 == 0:\n",
    "                encoders.append(conv[i])\n",
    "            else:\n",
    "                decoders.append(conv[i])\n",
    "\n",
    "\n",
    "    TESTSET_SIZE = 30000\n",
    "    train_enc = open(path + 'train.enc','w')\n",
    "    train_dec = open(path + 'train.dec','w')\n",
    "    test_enc  = open(path + 'test.enc', 'w')\n",
    "    test_dec  = open(path + 'test.dec', 'w')\n",
    "\n",
    "    # choose 30,000 (TESTSET_SIZE) items to put into testset\n",
    "    test_ids = random.sample([i for i in range(len(encoders))],TESTSET_SIZE)\n",
    "\n",
    "    for i in range(len(encoders)):\n",
    "        if i in test_ids:\n",
    "            test_enc.write(encoders[i]+'\\n')\n",
    "            test_dec.write(decoders[i]+ '\\n' )\n",
    "        else:\n",
    "            train_enc.write(encoders[i]+'\\n')\n",
    "            train_dec.write(decoders[i]+ '\\n' )\n",
    "        if i%10000 == 0:\n",
    "            print('\\n>> written %d lines' %(i))\n",
    "\n",
    "    # close files\n",
    "    train_enc.close()\n",
    "    train_dec.close()\n",
    "    test_enc.close()\n",
    "    test_dec.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "with open(path + \"train.enc\") as f:\n",
    "    for line in f.readlines():\n",
    "        questions.append(line)\n",
    "\n",
    "with open(path + \"train.dec\") as f:\n",
    "    for line in f.readlines():\n",
    "        answers.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for question in questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "for answer in answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
    "threshold_questions = 15\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold_questions:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "threshold_answers = 15\n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold_answers:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the last tokens to these two dictionaries\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1\n",
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the inverse dictionary of the answerswords2int dictionary\n",
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the End Of String token to the end of every answer\n",
    "for i in range(len(answers)):\n",
    "    answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no it is my fault  we did not have a proper introduction \n",
      "\n",
      "the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i ca not date until she does\n",
      " <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(questions[1])\n",
    "print(answers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating all the questions and the answers into integers\n",
    "# and Replacing all the words that were filtered out by <OUT> \n",
    "questions_into_int = []\n",
    "for question in questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)\n",
    "answers_into_int = []\n",
    "for answer in answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 15, 16, 17, 18, 19, 1, 20, 21, 22, 23, 24, 2682]\n",
      "[13, 500, 17, 2682, 52, 121, 56, 13, 1684, 46, 23, 1204, 2682, 2682, 46, 1777, 18, 247, 52, 164, 21, 34, 256, 42, 115, 2681]\n"
     ]
    }
   ],
   "source": [
    "print(questions_into_int[1])\n",
    "print(answers_into_int[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating placeholders for the inputs and the targets\n",
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the targets\n",
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
    "    return preprocessed_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Encoder RNN\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                                    cell_bw = encoder_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = rnn_inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the training set\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                              training_decoder_function,\n",
    "                                                                                                              decoder_embedded_input,\n",
    "                                                                                                              sequence_length,\n",
    "                                                                                                              scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)\n",
    "\n",
    "# Decoding the test/validation set\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                                test_decoder_function,\n",
    "                                                                                                                scope = decoding_scope)\n",
    "    return test_predictions\n",
    "\n",
    "# Creating the Decoder RNN\n",
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                   decoder_cell,\n",
    "                                                   decoder_embedded_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_function,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                           decoder_cell,\n",
    "                                           decoder_embeddings_matrix,\n",
    "                                           word2int['<SOS>'],\n",
    "                                           word2int['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_function,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    return training_predictions, test_predictions\n",
    "\n",
    "#Now that we have both the encoder RNN and the decoder RNN we'll use them to build our seq2seq model.\n",
    "\n",
    "# Building the seq2seq model\n",
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_embedding_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "rnn_size = 512\n",
    "num_layers = 5\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a session\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Loading the model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()\n",
    "\n",
    "# Setting the sequence length\n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
    "\n",
    "# Getting the shape of the inputs tensor\n",
    "input_shape = tf.shape(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting the training and test predictions\n",
    "\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerswords2int),\n",
    "                                                       len(questionswords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questionswords2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Loss Error, the Optimizer and Gradient Clipping\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the sequences with the <PAD> token\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int['<PAD>']]* (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into batches of questions and answers\n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the questions and answers into training and validation sets\n",
    "training_validation_split = int(len(questions) * 0.15)\n",
    "training_questions = questions_into_int[training_validation_split:]\n",
    "training_answers = answers_into_int[training_validation_split:]\n",
    "validation_questions = questions_into_int[:training_validation_split]\n",
    "validation_answers = answers_into_int[:training_validation_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 1000\n",
    "checkpoint = path + \"chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.8862514\n",
      "Epoch:   1/10, Batch:    0/161, Training Loss Error:  0.079, Training Time on 100 Batches: 124 seconds\n",
      "5.617066\n",
      "5.2972474\n",
      "3.2152195\n",
      "4.3598475\n",
      "3.168849\n",
      "2.9504247\n",
      "3.2844799\n",
      "2.4666042\n",
      "2.4919713\n",
      "2.5107875\n",
      "2.7537305\n",
      "2.493053\n",
      "2.5523977\n",
      "2.3941803\n",
      "2.6658041\n",
      "2.5992239\n",
      "2.3773713\n",
      "2.6943977\n",
      "2.6216278\n",
      "2.3781357\n",
      "2.5950842\n",
      "2.7951267\n",
      "2.2950833\n",
      "2.360455\n",
      "2.1624236\n",
      "2.1825795\n",
      "2.281942\n",
      "2.0776675\n",
      "2.3054476\n",
      "2.261247\n",
      "2.0158615\n",
      "1.9809529\n",
      "2.0945783\n",
      "2.016881\n",
      "2.683223\n",
      "2.1090534\n",
      "2.4279785\n",
      "2.1707423\n",
      "2.3007677\n",
      "1.8343419\n",
      "2.118326\n",
      "1.837468\n",
      "2.4132328\n",
      "2.3157463\n",
      "2.1947303\n",
      "3.0831695\n",
      "2.378959\n",
      "2.1718001\n",
      "2.1070669\n",
      "2.150984\n",
      "2.7158237\n",
      "1.9260279\n",
      "2.258096\n",
      "2.0943503\n",
      "2.2563376\n",
      "2.279701\n",
      "2.1321602\n",
      "2.0782459\n",
      "2.4891722\n",
      "2.4079847\n",
      "1.912127\n",
      "2.3412812\n",
      "2.4223092\n",
      "2.2449982\n",
      "1.889391\n",
      "2.1587431\n",
      "2.3004515\n",
      "2.625587\n",
      "2.3609798\n",
      "1.9253596\n",
      "2.4199238\n",
      "2.2198138\n",
      "2.0683856\n",
      "2.184271\n",
      "2.2191231\n",
      "1.9968404\n",
      "2.2704563\n",
      "1.8054627\n",
      "2.2975576\n",
      "Validation Loss Error:  2.077, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "2.417588\n",
      "2.8303742\n",
      "2.0055592\n",
      "2.4427907\n",
      "2.3653734\n",
      "2.291954\n",
      "2.049908\n",
      "2.420279\n",
      "2.1558025\n",
      "2.3243897\n",
      "2.069539\n",
      "2.0499036\n",
      "2.226845\n",
      "2.430107\n",
      "2.1569488\n",
      "2.1572344\n",
      "2.295839\n",
      "2.1124587\n",
      "1.9726567\n",
      "2.1534703\n",
      "2.203018\n",
      "Epoch:   1/10, Batch:  100/161, Training Loss Error:  2.400, Training Time on 100 Batches: 87 seconds\n",
      "2.128453\n",
      "1.7833581\n",
      "2.2255461\n",
      "2.0320203\n",
      "2.2140205\n",
      "2.0481594\n",
      "1.9834816\n",
      "2.1167681\n",
      "2.341755\n",
      "2.4390173\n",
      "2.0346375\n",
      "2.491536\n",
      "2.0606642\n",
      "1.974358\n",
      "2.1002908\n",
      "2.482232\n",
      "2.2143238\n",
      "2.2302058\n",
      "2.316267\n",
      "2.3451312\n",
      "2.1532712\n",
      "2.2262657\n",
      "2.487115\n",
      "2.1757917\n",
      "1.8817948\n",
      "2.1380484\n",
      "2.3153775\n",
      "2.4759774\n",
      "2.362797\n",
      "1.900955\n",
      "2.1568992\n",
      "2.2675803\n",
      "1.9284312\n",
      "2.2257864\n",
      "2.0117207\n",
      "2.5480778\n",
      "2.1323898\n",
      "1.7507945\n",
      "2.3275518\n",
      "1.8769349\n",
      "1.9934934\n",
      "2.0790226\n",
      "2.2592928\n",
      "2.2208958\n",
      "2.3059654\n",
      "2.1703668\n",
      "2.3142169\n",
      "2.49072\n",
      "2.2296457\n",
      "2.0815382\n",
      "2.4877927\n",
      "2.1944058\n",
      "2.060103\n",
      "2.2045696\n",
      "2.0709703\n",
      "1.8237305\n",
      "2.0210793\n",
      "2.3497794\n",
      "Validation Loss Error:  2.076, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.9562337\n",
      "2.2137856\n",
      "2.6830313\n",
      "Epoch:   2/10, Batch:    0/161, Training Loss Error:  1.331, Training Time on 100 Batches: 87 seconds\n",
      "2.2328222\n",
      "2.3208747\n",
      "2.2672508\n",
      "1.9249644\n",
      "2.468464\n",
      "2.245322\n",
      "2.3481576\n",
      "1.9314209\n",
      "2.0146003\n",
      "2.0570705\n",
      "2.3282075\n",
      "2.144993\n",
      "2.256818\n",
      "2.1419852\n",
      "2.3758085\n",
      "2.341222\n",
      "2.1439528\n",
      "2.482279\n",
      "2.4535801\n",
      "2.1687322\n",
      "2.434156\n",
      "2.647004\n",
      "2.1955726\n",
      "2.2473323\n",
      "2.062081\n",
      "2.0934124\n",
      "2.1774259\n",
      "2.004884\n",
      "2.2294242\n",
      "2.1759577\n",
      "1.9408425\n",
      "1.8996793\n",
      "2.02162\n",
      "1.9321196\n",
      "2.572254\n",
      "2.0341718\n",
      "2.3645337\n",
      "2.095469\n",
      "2.2360744\n",
      "1.7737013\n",
      "2.064296\n",
      "1.7897459\n",
      "2.332888\n",
      "2.2342854\n",
      "2.1362846\n",
      "3.0014668\n",
      "2.3114622\n",
      "2.1051774\n",
      "2.047613\n",
      "2.1033087\n",
      "2.6818979\n",
      "1.8771037\n",
      "2.200683\n",
      "2.03392\n",
      "2.205379\n",
      "2.2349331\n",
      "2.0888824\n",
      "2.0282354\n",
      "2.4280581\n",
      "2.3496149\n",
      "1.8683108\n",
      "2.294138\n",
      "2.3720388\n",
      "2.1984556\n",
      "1.857549\n",
      "2.1253855\n",
      "2.2490768\n",
      "2.5720572\n",
      "2.3202763\n",
      "1.8849704\n",
      "2.3878126\n",
      "2.1730392\n",
      "2.0216732\n",
      "2.1524656\n",
      "2.1803396\n",
      "1.9595217\n",
      "2.223363\n",
      "1.7809248\n",
      "2.2457776\n",
      "Validation Loss Error:  2.068, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "2.376739\n",
      "2.791545\n",
      "1.961899\n",
      "2.3893547\n",
      "2.331442\n",
      "2.257671\n",
      "2.0244455\n",
      "2.3775303\n",
      "2.1251237\n",
      "2.2880359\n",
      "2.0390124\n",
      "2.0181818\n",
      "2.1953046\n",
      "2.4024246\n",
      "2.1242282\n",
      "2.1178844\n",
      "2.264683\n",
      "2.0680478\n",
      "1.947979\n",
      "2.105877\n",
      "2.156047\n",
      "Epoch:   2/10, Batch:  100/161, Training Loss Error:  2.193, Training Time on 100 Batches: 87 seconds\n",
      "2.0973866\n",
      "1.7456824\n",
      "2.1758626\n",
      "1.98379\n",
      "2.1599307\n",
      "1.9998956\n",
      "1.9315009\n",
      "2.0569782\n",
      "2.261261\n",
      "2.3388813\n",
      "1.940179\n",
      "2.37557\n",
      "1.945969\n",
      "1.8785907\n",
      "1.9388208\n",
      "2.3188663\n",
      "2.0173922\n",
      "2.0010798\n",
      "2.082154\n",
      "2.293168\n",
      "2.025227\n",
      "2.1956136\n",
      "2.3530302\n",
      "2.0792043\n",
      "1.7644193\n",
      "2.0607862\n",
      "2.165029\n",
      "2.252905\n",
      "2.1229644\n",
      "1.7211105\n",
      "1.9511352\n",
      "2.02065\n",
      "1.687414\n",
      "1.9862723\n",
      "1.7411932\n",
      "2.184313\n",
      "1.8603994\n",
      "1.4782445\n",
      "2.0328326\n",
      "1.5660772\n",
      "1.6670538\n",
      "1.7464962\n",
      "1.8752681\n",
      "1.8834063\n",
      "1.9255742\n",
      "1.7982961\n",
      "1.9538591\n",
      "2.0906162\n",
      "1.8336413\n",
      "1.7166419\n",
      "2.1001508\n",
      "1.8061422\n",
      "1.7415214\n",
      "1.819551\n",
      "1.6955721\n",
      "1.5057127\n",
      "1.6556079\n",
      "1.9300387\n",
      "Validation Loss Error:  1.724, Batch Validation Time: 9 seconds\n",
      "I speak better now!!\n",
      "1.6343759\n",
      "1.8193884\n",
      "2.246142\n",
      "Epoch:   3/10, Batch:    0/161, Training Loss Error:  1.192, Training Time on 100 Batches: 91 seconds\n",
      "1.8390595\n",
      "1.9485296\n",
      "1.848025\n",
      "1.5391643\n",
      "2.042642\n",
      "1.829201\n",
      "1.9184656\n",
      "1.5743811\n",
      "1.6505462\n",
      "1.6897097\n",
      "1.938941\n",
      "1.7761234\n",
      "1.8482918\n",
      "1.7679845\n",
      "1.9635385\n",
      "1.9338256\n",
      "1.7768302\n",
      "2.0736463\n",
      "1.9994911\n",
      "1.8015729\n",
      "2.0075786\n",
      "2.2161999\n",
      "1.8075651\n",
      "1.8212303\n",
      "1.6759168\n",
      "1.7300485\n",
      "1.7743126\n",
      "1.6281884\n",
      "1.8036828\n",
      "1.7821511\n",
      "1.5882008\n",
      "1.507976\n",
      "1.6601019\n",
      "1.5955749\n",
      "2.104893\n",
      "1.6839958\n",
      "1.9587462\n",
      "1.7072452\n",
      "1.8364984\n",
      "1.4544703\n",
      "1.7102968\n",
      "1.4646617\n",
      "1.8123839\n",
      "1.8063755\n",
      "1.7335095\n",
      "2.4750807\n",
      "1.8860835\n",
      "1.5998112\n",
      "1.5594351\n",
      "1.7177289\n",
      "2.1446898\n",
      "1.515755\n",
      "1.7704166\n",
      "1.6457627\n",
      "1.7650013\n",
      "1.8267848\n",
      "1.7273333\n",
      "1.6271292\n",
      "2.0013266\n",
      "1.9475455\n",
      "1.5376986\n",
      "1.8529011\n",
      "1.9098607\n",
      "1.8109543\n",
      "1.5359936\n",
      "1.7452496\n",
      "1.8703551\n",
      "2.1317377\n",
      "1.9054177\n",
      "1.4814017\n",
      "1.9809933\n",
      "1.7744244\n",
      "1.6382345\n",
      "1.7662735\n",
      "1.7962242\n",
      "1.5953473\n",
      "1.8025006\n",
      "1.4471153\n",
      "1.8562593\n",
      "Validation Loss Error:  1.714, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.948526\n",
      "2.3387542\n",
      "1.5629269\n",
      "1.9800098\n",
      "1.9291719\n",
      "1.8441885\n",
      "1.6680132\n",
      "1.9353268\n",
      "1.7546004\n",
      "1.8480529\n",
      "1.6722752\n",
      "1.6416948\n",
      "1.8044178\n",
      "1.9786296\n",
      "1.7629914\n",
      "1.7379806\n",
      "1.8535222\n",
      "1.729425\n",
      "1.5996081\n",
      "1.7354652\n",
      "1.778414\n",
      "Epoch:   3/10, Batch:  100/161, Training Loss Error:  1.794, Training Time on 100 Batches: 89 seconds\n",
      "1.7298936\n",
      "1.4169931\n",
      "1.8152038\n",
      "1.6323234\n",
      "1.8079463\n",
      "1.637768\n",
      "1.607448\n",
      "1.7447072\n",
      "1.9049212\n",
      "1.9866985\n",
      "1.6421953\n",
      "2.063008\n",
      "1.6483874\n",
      "1.5663987\n",
      "1.6817122\n",
      "2.0397456\n",
      "1.7975215\n",
      "1.8282034\n",
      "1.8887118\n",
      "1.9080725\n",
      "1.7333194\n",
      "1.8315058\n",
      "2.027003\n",
      "1.7598239\n",
      "1.508034\n",
      "1.7293403\n",
      "1.8784424\n",
      "2.025283\n",
      "1.9260628\n",
      "1.5027301\n",
      "1.7563211\n",
      "1.8664073\n",
      "1.5752914\n",
      "1.8099355\n",
      "1.6299788\n",
      "2.062769\n",
      "1.7240521\n",
      "1.3705349\n",
      "1.9228944\n",
      "1.4836574\n",
      "1.6003721\n",
      "1.6865792\n",
      "1.8068147\n",
      "1.8109292\n",
      "1.8703822\n",
      "1.750774\n",
      "1.9061449\n",
      "2.0502052\n",
      "1.7998885\n",
      "1.6889664\n",
      "2.059192\n",
      "1.7682898\n",
      "1.7025125\n",
      "1.7961222\n",
      "1.6745571\n",
      "1.4925605\n",
      "1.6393071\n",
      "1.9190673\n",
      "Validation Loss Error:  1.712, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.6170489\n",
      "1.8036901\n",
      "2.2182443\n",
      "Epoch:   4/10, Batch:    0/161, Training Loss Error:  1.081, Training Time on 100 Batches: 90 seconds\n",
      "1.8246888\n",
      "1.9352041\n",
      "1.831514\n",
      "1.518846\n",
      "2.0273435\n",
      "1.8131311\n",
      "1.9094101\n",
      "1.5618905\n",
      "1.6413397\n",
      "1.6819308\n",
      "1.9299088\n",
      "1.7528664\n",
      "1.8399385\n",
      "1.7517546\n",
      "1.950158\n",
      "1.9266354\n",
      "1.76616\n",
      "2.0607057\n",
      "1.9854895\n",
      "1.7881172\n",
      "1.9933004\n",
      "2.1976476\n",
      "1.7939208\n",
      "1.8073548\n",
      "1.6647447\n",
      "1.7232556\n",
      "1.7628558\n",
      "1.6207458\n",
      "1.7906979\n",
      "1.7701122\n",
      "1.5808476\n",
      "1.5042119\n",
      "1.65086\n",
      "1.5803252\n",
      "2.0976079\n",
      "1.6718568\n",
      "1.9447709\n",
      "1.70148\n",
      "1.8285571\n",
      "1.4465988\n",
      "1.7059927\n",
      "1.4604547\n",
      "1.8006278\n",
      "1.7902931\n",
      "1.7247946\n",
      "2.4600015\n",
      "1.8693242\n",
      "1.5885094\n",
      "1.5568093\n",
      "1.7128373\n",
      "2.139417\n",
      "1.5066696\n",
      "1.760735\n",
      "1.6340083\n",
      "1.7648343\n",
      "1.8163105\n",
      "1.7182469\n",
      "1.6227912\n",
      "1.9888297\n",
      "1.9341325\n",
      "1.527727\n",
      "1.8424375\n",
      "1.9022675\n",
      "1.7980961\n",
      "1.5285019\n",
      "1.7344085\n",
      "1.8562542\n",
      "2.1176314\n",
      "1.8900865\n",
      "1.4750943\n",
      "1.9739838\n",
      "1.7625996\n",
      "1.6318275\n",
      "1.7522291\n",
      "1.7895048\n",
      "1.585982\n",
      "1.7802501\n",
      "1.4358969\n",
      "1.8443103\n",
      "Validation Loss Error:  1.705, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.9291977\n",
      "2.3290281\n",
      "1.5485177\n",
      "1.9708011\n",
      "1.9179074\n",
      "1.831441\n",
      "1.657763\n",
      "1.9209805\n",
      "1.7372013\n",
      "1.833686\n",
      "1.6518323\n",
      "1.6314727\n",
      "1.7979065\n",
      "1.9580524\n",
      "1.749996\n",
      "1.7233725\n",
      "1.8393576\n",
      "1.7090628\n",
      "1.5846492\n",
      "1.721509\n",
      "1.758418\n",
      "Epoch:   4/10, Batch:  100/161, Training Loss Error:  1.782, Training Time on 100 Batches: 89 seconds\n",
      "1.72226\n",
      "1.4013654\n",
      "1.7948979\n",
      "1.6162711\n",
      "1.7810742\n",
      "1.627688\n",
      "1.589318\n",
      "1.7277391\n",
      "1.8867068\n",
      "1.9701244\n",
      "1.6245023\n",
      "2.0348945\n",
      "1.6297568\n",
      "1.5448908\n",
      "1.6538104\n",
      "2.0157778\n",
      "1.7697877\n",
      "1.7942878\n",
      "1.8570515\n",
      "1.8753387\n",
      "1.7091554\n",
      "1.8032976\n",
      "1.9993029\n",
      "1.7357912\n",
      "1.4825287\n",
      "1.6954243\n",
      "1.8478519\n",
      "1.9837625\n",
      "1.8933452\n",
      "1.4820517\n",
      "1.7218747\n",
      "1.8343041\n",
      "1.5431931\n",
      "1.7783158\n",
      "1.6049112\n",
      "2.024658\n",
      "1.6893644\n",
      "1.336987\n",
      "1.8855662\n",
      "1.4518005\n",
      "1.5668023\n",
      "1.6516607\n",
      "1.7705204\n",
      "1.7789361\n",
      "1.831064\n",
      "1.7163767\n",
      "1.8670745\n",
      "2.0064263\n",
      "1.7600539\n",
      "1.6425167\n",
      "1.9981455\n",
      "1.7295526\n",
      "1.6640327\n",
      "1.7547729\n",
      "1.6268057\n",
      "1.4430828\n",
      "1.5908972\n",
      "1.876359\n",
      "Validation Loss Error:  1.662, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.5743593\n",
      "1.7672659\n",
      "2.1737533\n",
      "Epoch:   5/10, Batch:    0/161, Training Loss Error:  1.062, Training Time on 100 Batches: 88 seconds\n",
      "1.7692956\n",
      "1.8938391\n",
      "1.7838637\n",
      "1.4884176\n",
      "1.9727911\n",
      "1.7700826\n",
      "1.8567106\n",
      "1.5270656\n",
      "1.5811789\n",
      "1.6385357\n",
      "1.8845773\n",
      "1.7036239\n",
      "1.79519\n",
      "1.7078503\n",
      "1.8936871\n",
      "1.8762273\n",
      "1.7173879\n",
      "2.010533\n",
      "1.9408735\n",
      "1.7417935\n",
      "1.9280522\n",
      "2.14337\n",
      "1.7421874\n",
      "1.7396395\n",
      "1.6155303\n",
      "1.6870441\n",
      "1.7087661\n",
      "1.5653198\n",
      "1.7389517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7191671\n",
      "1.5358166\n",
      "1.4544157\n",
      "1.6044387\n",
      "1.5393308\n",
      "2.0364034\n",
      "1.6223773\n",
      "1.8811123\n",
      "1.6471316\n",
      "1.7697561\n",
      "1.4038365\n",
      "1.6495581\n",
      "1.4010878\n",
      "1.7421412\n",
      "1.7365495\n",
      "1.673135\n",
      "2.38072\n",
      "1.800456\n",
      "1.5430545\n",
      "1.4933891\n",
      "1.6570352\n",
      "2.061416\n",
      "1.4512265\n",
      "1.6931758\n",
      "1.577569\n",
      "1.692321\n",
      "1.7608318\n",
      "1.6818393\n",
      "1.5554351\n",
      "1.9247929\n",
      "1.873421\n",
      "1.4703051\n",
      "1.784062\n",
      "1.8391343\n",
      "1.7354863\n",
      "1.4732386\n",
      "1.682557\n",
      "1.7930336\n",
      "2.045377\n",
      "1.8254943\n",
      "1.4224519\n",
      "1.9068512\n",
      "1.7144814\n",
      "1.5795466\n",
      "1.6808002\n",
      "1.7256104\n",
      "1.5296929\n",
      "1.7156267\n",
      "1.3834436\n",
      "1.7832896\n",
      "Validation Loss Error:  1.638, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.8700981\n",
      "2.2511718\n",
      "1.4987217\n",
      "1.8993322\n",
      "1.8542337\n",
      "1.7612207\n",
      "1.5992143\n",
      "1.8603944\n",
      "1.6639906\n",
      "1.7629876\n",
      "1.5946159\n",
      "1.5627254\n",
      "1.7485598\n",
      "1.8798001\n",
      "1.6876163\n",
      "1.6594183\n",
      "1.7828647\n",
      "1.645161\n",
      "1.5321541\n",
      "1.6507171\n",
      "1.6825856\n",
      "Epoch:   5/10, Batch:  100/161, Training Loss Error:  1.725, Training Time on 100 Batches: 87 seconds\n",
      "1.6605706\n",
      "1.347811\n",
      "1.7229034\n",
      "1.5603935\n",
      "1.7222985\n",
      "1.5608934\n",
      "1.5385451\n",
      "1.6783557\n",
      "1.827502\n",
      "1.9169846\n",
      "1.5783813\n",
      "1.980781\n",
      "1.5819836\n",
      "1.4928946\n",
      "1.5969458\n",
      "1.9478015\n",
      "1.7117944\n",
      "1.7478485\n",
      "1.8000003\n",
      "1.8167819\n",
      "1.6606342\n",
      "1.7548817\n",
      "1.9395111\n",
      "1.6837217\n",
      "1.4453182\n",
      "1.6409897\n",
      "1.7778201\n",
      "1.9249905\n",
      "1.8501748\n",
      "1.4303392\n",
      "1.676553\n",
      "1.7897142\n",
      "1.5005107\n",
      "1.7292271\n",
      "1.5586002\n",
      "1.9845685\n",
      "1.6355051\n",
      "1.2947366\n",
      "1.841596\n",
      "1.4204067\n",
      "1.5319557\n",
      "1.6147127\n",
      "1.7311537\n",
      "1.7415942\n",
      "1.7866639\n",
      "1.6806711\n",
      "1.8123698\n",
      "1.939792\n",
      "1.7085663\n",
      "1.5988923\n",
      "1.938845\n",
      "1.6911126\n",
      "1.6195593\n",
      "1.713126\n",
      "1.5772889\n",
      "1.4048269\n",
      "1.557302\n",
      "1.8449677\n",
      "Validation Loss Error:  1.624, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.5391527\n",
      "1.7295992\n",
      "2.1206489\n",
      "Epoch:   6/10, Batch:    0/161, Training Loss Error:  1.032, Training Time on 100 Batches: 87 seconds\n",
      "1.7319406\n",
      "1.8470423\n",
      "1.7395225\n",
      "1.4541037\n",
      "1.9322091\n",
      "1.7291093\n",
      "1.8262323\n",
      "1.4790001\n",
      "1.5292317\n",
      "1.5888447\n",
      "1.8340188\n",
      "1.6663971\n",
      "1.754328\n",
      "1.6564705\n",
      "1.8507236\n",
      "1.8448373\n",
      "1.6760937\n",
      "1.963561\n",
      "1.8964545\n",
      "1.7014431\n",
      "1.8749425\n",
      "2.083338\n",
      "1.699862\n",
      "1.6983179\n",
      "1.5725844\n",
      "1.632462\n",
      "1.6607684\n",
      "1.537054\n",
      "1.7072585\n",
      "1.6791439\n",
      "1.5030847\n",
      "1.4164329\n",
      "1.5711576\n",
      "1.5061413\n",
      "1.9843947\n",
      "1.5860673\n",
      "1.8411063\n",
      "1.6078236\n",
      "1.7329034\n",
      "1.3642284\n",
      "1.6151155\n",
      "1.3752033\n",
      "1.698528\n",
      "1.6979707\n",
      "1.629277\n",
      "2.3124108\n",
      "1.7678887\n",
      "1.5100452\n",
      "1.4615793\n",
      "1.6232642\n",
      "2.0094347\n",
      "1.4225669\n",
      "1.6473309\n",
      "1.5391443\n",
      "1.6571745\n",
      "1.7271028\n",
      "1.6545694\n",
      "1.5231699\n",
      "1.8835524\n",
      "1.8394266\n",
      "1.4391624\n",
      "1.7506709\n",
      "1.8053226\n",
      "1.7011057\n",
      "1.4435593\n",
      "1.6544349\n",
      "1.7510331\n",
      "2.0070872\n",
      "1.8017812\n",
      "1.3956593\n",
      "1.8738935\n",
      "1.692215\n",
      "1.5533553\n",
      "1.6431751\n",
      "1.6968766\n",
      "1.4995474\n",
      "1.6798337\n",
      "1.3608811\n",
      "1.7459217\n",
      "Validation Loss Error:  1.605, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.8193811\n",
      "2.2173157\n",
      "1.4676625\n",
      "1.8561655\n",
      "1.8213524\n",
      "1.7201333\n",
      "1.572592\n",
      "1.8189086\n",
      "1.6377778\n",
      "1.7328165\n",
      "1.5547961\n",
      "1.5389541\n",
      "1.7165598\n",
      "1.8468196\n",
      "1.6529969\n",
      "1.6197484\n",
      "1.7498251\n",
      "1.6043613\n",
      "1.5020238\n",
      "1.6113433\n",
      "1.6497353\n",
      "Epoch:   6/10, Batch:  100/161, Training Loss Error:  1.688, Training Time on 100 Batches: 87 seconds\n",
      "1.6299286\n",
      "1.3248525\n",
      "1.6931995\n",
      "1.5302092\n",
      "1.6924789\n",
      "1.5361947\n",
      "1.5048339\n",
      "1.6464386\n",
      "1.7941303\n",
      "1.8878224\n",
      "1.5497469\n",
      "1.9359658\n",
      "1.5574286\n",
      "1.4664102\n",
      "1.5701457\n",
      "1.920513\n",
      "1.6690013\n",
      "1.7108973\n",
      "1.7605029\n",
      "1.78386\n",
      "1.630979\n",
      "1.7177782\n",
      "1.9099145\n",
      "1.6515672\n",
      "1.4190751\n",
      "1.6052076\n",
      "1.7408736\n",
      "1.8901376\n",
      "1.8144876\n",
      "1.4079083\n",
      "1.644571\n",
      "1.759637\n",
      "1.4785309\n",
      "1.6957153\n",
      "1.536995\n",
      "1.9647443\n",
      "1.6034646\n",
      "1.2756326\n",
      "1.8075767\n",
      "1.4061393\n",
      "1.5041904\n",
      "1.5857035\n",
      "1.7040373\n",
      "1.7021717\n",
      "1.760011\n",
      "1.654465\n",
      "1.7784187\n",
      "1.9102348\n",
      "1.6796038\n",
      "1.5746185\n",
      "1.903097\n",
      "1.6605748\n",
      "1.5970443\n",
      "1.6772052\n",
      "1.5461292\n",
      "1.379052\n",
      "1.5338621\n",
      "1.8094742\n",
      "Validation Loss Error:  1.597, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.5111477\n",
      "1.7056044\n",
      "2.0822766\n",
      "Epoch:   7/10, Batch:    0/161, Training Loss Error:  1.014, Training Time on 100 Batches: 89 seconds\n",
      "1.6985341\n",
      "1.8169854\n",
      "1.7037847\n",
      "1.42938\n",
      "1.8908767\n",
      "1.7036108\n",
      "1.7819264\n",
      "1.4491278\n",
      "1.504746\n",
      "1.5603048\n",
      "1.8024122\n",
      "1.6437535\n",
      "1.7269489\n",
      "1.6277595\n",
      "1.8200275\n",
      "1.8166475\n",
      "1.6426914\n",
      "1.9416276\n",
      "1.8535042\n",
      "1.6751001\n",
      "1.8447872\n",
      "2.046778\n",
      "1.6674784\n",
      "1.6558896\n",
      "1.5313134\n",
      "1.6112605\n",
      "1.6287342\n",
      "1.5035478\n",
      "1.6789001\n",
      "1.6516913\n",
      "1.4761487\n",
      "1.3874698\n",
      "1.5444955\n",
      "1.4895687\n",
      "1.9545853\n",
      "1.564851\n",
      "1.8146424\n",
      "1.5816556\n",
      "1.7072283\n",
      "1.3386456\n",
      "1.5949169\n",
      "1.3515723\n",
      "1.6712494\n",
      "1.6663299\n",
      "1.6055496\n",
      "2.2846696\n",
      "1.7360746\n",
      "1.4921007\n",
      "1.4360363\n",
      "1.6015968\n",
      "1.9810853\n",
      "1.4109356\n",
      "1.6215713\n",
      "1.5105244\n",
      "1.6295545\n",
      "1.7067752\n",
      "1.6334288\n",
      "1.4967988\n",
      "1.8582745\n",
      "1.8083332\n",
      "1.4108466\n",
      "1.7214586\n",
      "1.7746774\n",
      "1.6787322\n",
      "1.4264455\n",
      "1.6317708\n",
      "1.7210107\n",
      "1.9750737\n",
      "1.7733061\n",
      "1.3711036\n",
      "1.8403299\n",
      "1.668135\n",
      "1.5302362\n",
      "1.613516\n",
      "1.6616064\n",
      "1.4723598\n",
      "1.6546613\n",
      "1.3417802\n",
      "1.7301707\n",
      "Validation Loss Error:  1.585, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.7924762\n",
      "2.187953\n",
      "1.443963\n",
      "1.8288698\n",
      "1.7935637\n",
      "1.6989304\n",
      "1.5531496\n",
      "1.7954481\n",
      "1.6072729\n",
      "1.7036608\n",
      "1.5321478\n",
      "1.5151979\n",
      "1.6925426\n",
      "1.8102959\n",
      "1.6338288\n",
      "1.5916758\n",
      "1.7292533\n",
      "1.5822905\n",
      "1.4838754\n",
      "1.5937104\n",
      "1.6253859\n",
      "Epoch:   7/10, Batch:  100/161, Training Loss Error:  1.661, Training Time on 100 Batches: 87 seconds\n",
      "1.5933676\n",
      "1.3062456\n",
      "1.6674149\n",
      "1.5081877\n",
      "1.668433\n",
      "1.5124522\n",
      "1.4850914\n",
      "1.6203222\n",
      "1.7669294\n",
      "1.8612006\n",
      "1.5370967\n",
      "1.9093634\n",
      "1.5392523\n",
      "1.4483927\n",
      "1.5358084\n",
      "1.8914135\n",
      "1.6442666\n",
      "1.6784686\n",
      "1.7293123\n",
      "1.7504561\n",
      "1.6088024\n",
      "1.692695\n",
      "1.877039\n",
      "1.6211853\n",
      "1.3970419\n",
      "1.5867758\n",
      "1.7194833\n",
      "1.8656557\n",
      "1.7827281\n",
      "1.3848397\n",
      "1.6225832\n",
      "1.730672\n",
      "1.4580462\n",
      "1.673773\n",
      "1.5196162\n",
      "1.9379252\n",
      "1.5753914\n",
      "1.2590624\n",
      "1.7959493\n",
      "1.3828328\n",
      "1.4845841\n",
      "1.5606029\n",
      "1.6774764\n",
      "1.6867052\n",
      "1.7352972\n",
      "1.6373267\n",
      "1.7580267\n",
      "1.8801117\n",
      "1.6602857\n",
      "1.550352\n",
      "1.8792362\n",
      "1.6321071\n",
      "1.5788451\n",
      "1.6528882\n",
      "1.5251015\n",
      "1.361717\n",
      "1.5129309\n",
      "1.7973126\n",
      "Validation Loss Error:  1.576, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.4934689\n",
      "1.6914009\n",
      "2.0551507\n",
      "Epoch:   8/10, Batch:    0/161, Training Loss Error:  1.000, Training Time on 100 Batches: 88 seconds\n",
      "1.674934\n",
      "1.7943763\n",
      "1.6769147\n",
      "1.411281\n",
      "1.8685758\n",
      "1.6773539\n",
      "1.7581165\n",
      "1.4367826\n",
      "1.4788982\n",
      "1.5280834\n",
      "1.77962\n",
      "1.6167037\n",
      "1.7017679\n",
      "1.6048847\n",
      "1.8047458\n",
      "1.7993963\n",
      "1.6281949\n",
      "1.9072403\n",
      "1.8318003\n",
      "1.6571686\n",
      "1.82348\n",
      "2.0184133\n",
      "1.6494021\n",
      "1.63569\n",
      "1.5100443\n",
      "1.5921838\n",
      "1.6065432\n",
      "1.4832677\n",
      "1.6523929\n",
      "1.6233717\n",
      "1.4592521\n",
      "1.3691077\n",
      "1.5181507\n",
      "1.463319\n",
      "1.9203669\n",
      "1.5359652\n",
      "1.7910019\n",
      "1.5554991\n",
      "1.6751103\n",
      "1.3241884\n",
      "1.5630109\n",
      "1.331533\n",
      "1.6472346\n",
      "1.6339613\n",
      "1.5868086\n",
      "2.2594829\n",
      "1.7092803\n",
      "1.4744673\n",
      "1.4103755\n",
      "1.5797758\n",
      "1.9526613\n",
      "1.3935325\n",
      "1.5985403\n",
      "1.4884756\n",
      "1.6034397\n",
      "1.6750833\n",
      "1.6103458\n",
      "1.4701135\n",
      "1.8377403\n",
      "1.7854775\n",
      "1.3820546\n",
      "1.6896527\n",
      "1.7484144\n",
      "1.6535865\n",
      "1.4043119\n",
      "1.6162956\n",
      "1.6972495\n",
      "1.9484504\n",
      "1.7528018\n",
      "1.3504379\n",
      "1.823232\n",
      "1.6524948\n",
      "1.5190631\n",
      "1.5921987\n",
      "1.6423056\n",
      "1.457372\n",
      "1.6336511\n",
      "1.326641\n",
      "1.7032428\n",
      "Validation Loss Error:  1.564, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.7617295\n",
      "2.153603\n",
      "1.4216172\n",
      "1.8084186\n",
      "1.7791512\n",
      "1.6777595\n",
      "1.5453852\n",
      "1.7754128\n",
      "1.5838687\n",
      "1.683807\n",
      "1.5132465\n",
      "1.4958309\n",
      "1.6708877\n",
      "1.7875637\n",
      "1.6130532\n",
      "1.5666931\n",
      "1.7015752\n",
      "1.5557566\n",
      "1.4669778\n",
      "1.5632689\n",
      "1.6037434\n",
      "Epoch:   8/10, Batch:  100/161, Training Loss Error:  1.638, Training Time on 100 Batches: 89 seconds\n",
      "1.5740517\n",
      "1.2817715\n",
      "1.6382468\n",
      "1.491255\n",
      "1.6455623\n",
      "1.4959958\n",
      "1.4574293\n",
      "1.5989234\n",
      "1.7474712\n",
      "1.841243\n",
      "1.5142368\n",
      "1.8874215\n",
      "1.5245057\n",
      "1.4299654\n",
      "1.5063218\n",
      "1.8585236\n",
      "1.6259179\n",
      "1.6550443\n",
      "1.6988965\n",
      "1.7216502\n",
      "1.5840569\n",
      "1.6709373\n",
      "1.8390313\n",
      "1.5996279\n",
      "1.3836564\n",
      "1.562522\n",
      "1.7017503\n",
      "1.8310701\n",
      "1.7638217\n",
      "1.3742048\n",
      "1.6000271\n",
      "1.703751\n",
      "1.4357266\n",
      "1.6517888\n",
      "1.4995039\n",
      "1.9064364\n",
      "1.5558038\n",
      "1.2408125\n",
      "1.7729743\n",
      "1.3692515\n",
      "1.4699216\n",
      "1.5394797\n",
      "1.6584693\n",
      "1.665042\n",
      "1.7133975\n",
      "1.6187499\n",
      "1.7245013\n",
      "1.8655354\n",
      "1.642558\n",
      "1.5300424\n",
      "1.8539476\n",
      "1.6162207\n",
      "1.5599684\n",
      "1.6293736\n",
      "1.4981155\n",
      "1.3360919\n",
      "1.498222\n",
      "1.7701691\n",
      "Validation Loss Error:  1.555, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.4759871\n",
      "1.6623597\n",
      "2.0140939\n",
      "Epoch:   9/10, Batch:    0/161, Training Loss Error:  0.986, Training Time on 100 Batches: 88 seconds\n",
      "1.6522342\n",
      "1.7706844\n",
      "1.6459261\n",
      "1.396683\n",
      "1.8306096\n",
      "1.6424543\n",
      "1.733305\n",
      "1.4097087\n",
      "1.4562887\n",
      "1.5083885\n",
      "1.760977\n",
      "1.60339\n",
      "1.6821612\n",
      "1.5881726\n",
      "1.7801172\n",
      "1.7751689\n",
      "1.6071995\n",
      "1.8879564\n",
      "1.8176605\n",
      "1.6310831\n",
      "1.791392\n",
      "1.9886009\n",
      "1.6254039\n",
      "1.602645\n",
      "1.4864657\n",
      "1.5737908\n",
      "1.5884287\n",
      "1.4604374\n",
      "1.6353471\n",
      "1.5998002\n",
      "1.4428835\n",
      "1.3488121\n",
      "1.5023121\n",
      "1.4469342\n",
      "1.8948964\n",
      "1.511597\n",
      "1.7760099\n",
      "1.5406604\n",
      "1.6518625\n",
      "1.3098246\n",
      "1.5417137\n",
      "1.3168706\n",
      "1.6172634\n",
      "1.6202995\n",
      "1.559816\n",
      "2.2276707\n",
      "1.6916323\n",
      "1.4440414\n",
      "1.3946062\n",
      "1.5589116\n",
      "1.9263389\n",
      "1.365835\n",
      "1.5759974\n",
      "1.4608462\n",
      "1.5793703\n",
      "1.6657794\n",
      "1.5936531\n",
      "1.4404004\n",
      "1.80708\n",
      "1.760822\n",
      "1.3687539\n",
      "1.6692826\n",
      "1.7299894\n",
      "1.6306446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3862754\n",
      "1.5845618\n",
      "1.6761101\n",
      "1.9263896\n",
      "1.7191637\n",
      "1.3304553\n",
      "1.8052671\n",
      "1.6316534\n",
      "1.4967133\n",
      "1.571353\n",
      "1.6187487\n",
      "1.4334177\n",
      "1.6090307\n",
      "1.3072065\n",
      "1.677396\n",
      "Validation Loss Error:  1.545, Batch Validation Time: 8 seconds\n",
      "I speak better now!!\n",
      "1.7413223\n",
      "2.122792\n",
      "1.4004749\n",
      "1.784185\n",
      "1.7578367\n",
      "1.6601355\n",
      "1.5242469\n",
      "1.7472802\n",
      "1.5557327\n",
      "1.6624727\n",
      "1.4850094\n",
      "1.4724233\n",
      "1.6540034\n",
      "1.761316\n",
      "1.5875505\n",
      "1.5469112\n",
      "1.6856465\n",
      "1.5360141\n",
      "1.4461958\n",
      "1.5463222\n",
      "1.5738672\n",
      "Epoch:   9/10, Batch:  100/161, Training Loss Error:  1.615, Training Time on 100 Batches: 88 seconds\n",
      "1.5544881\n",
      "1.2702503\n",
      "1.6196831\n",
      "1.4645222\n",
      "1.6238881\n",
      "1.4760706\n",
      "1.4398676\n",
      "1.5781877\n",
      "1.7238955\n",
      "1.8187854\n",
      "1.4965327\n",
      "1.8610823\n",
      "1.5041641\n",
      "1.4106965\n",
      "1.4892565\n",
      "1.8370339\n",
      "1.5934244\n",
      "1.6322145\n",
      "1.6739533\n",
      "1.7005147\n",
      "1.5601429\n",
      "1.643641\n",
      "1.8196589\n",
      "1.5664117\n",
      "1.3651301\n",
      "1.5351056\n",
      "1.6784868\n",
      "1.8027171\n",
      "1.747512\n",
      "1.3490235\n",
      "1.5776907\n",
      "1.6759783\n",
      "1.4141952\n",
      "1.6374688\n",
      "1.4831305\n",
      "1.8848504\n",
      "1.5316693\n",
      "1.2195606\n",
      "1.7517061\n",
      "1.3520027\n",
      "1.4496771\n",
      "1.5138204\n",
      "1.6350774\n",
      "1.6514362\n",
      "1.69641\n",
      "1.5950907\n",
      "1.7039694\n",
      "1.8375603\n",
      "1.6153865\n",
      "1.5119799\n",
      "1.8245497\n",
      "1.5901154\n",
      "1.5413646\n",
      "1.6055988\n",
      "1.4808217\n",
      "1.3173985\n",
      "1.465752\n",
      "1.7485822\n",
      "Validation Loss Error:  1.537, Batch Validation Time: 27 seconds\n",
      "I speak better now!!\n",
      "1.456223\n",
      "1.6430736\n",
      "1.9869655\n",
      "Epoch:  10/10, Batch:    0/161, Training Loss Error:  0.972, Training Time on 100 Batches: 258 seconds\n",
      "1.6268078\n",
      "1.7559075\n",
      "1.6355348\n",
      "1.3707132\n",
      "1.8128496\n",
      "1.6266962\n",
      "1.7061113\n",
      "1.3863972\n",
      "1.4271337\n",
      "1.4799913\n",
      "1.73627\n",
      "1.5725847\n",
      "1.6580749\n",
      "1.56419\n",
      "1.7659283\n",
      "1.7586536\n",
      "1.5825443\n",
      "1.872804\n",
      "1.7992599\n",
      "1.6055422\n",
      "1.7670648\n",
      "1.9648722\n",
      "1.6010238\n",
      "1.5798218\n",
      "1.4661214\n",
      "1.5558791\n",
      "1.5691963\n",
      "1.4436992\n",
      "1.6130697\n",
      "1.5894908\n",
      "1.4287086\n",
      "1.333338\n",
      "1.482246\n",
      "1.4322705\n",
      "1.8719337\n",
      "1.4899625\n",
      "1.7495557\n",
      "1.5153071\n",
      "1.630222\n",
      "1.2834265\n",
      "1.5141531\n",
      "1.2968786\n",
      "1.6037766\n",
      "1.602076\n",
      "1.5401356\n",
      "2.2017365\n",
      "1.6691859\n",
      "1.4275713\n",
      "1.3697041\n",
      "1.5420011\n",
      "1.897285\n",
      "1.3583881\n",
      "1.5532734\n",
      "1.436261\n",
      "1.5604193\n",
      "1.6467118\n",
      "1.577503\n",
      "1.4233593\n",
      "1.7810259\n",
      "1.7421818\n",
      "1.341254\n",
      "1.6500397\n",
      "1.709997\n",
      "1.6126347\n",
      "1.3717895\n",
      "1.5703585\n",
      "1.6532335\n",
      "1.8971549\n",
      "1.6969708\n",
      "1.3061726\n",
      "1.7637626\n",
      "1.6131257\n",
      "1.4774152\n",
      "1.5434021\n",
      "1.6020824\n",
      "1.4110435\n",
      "1.5907736\n",
      "1.2872616\n",
      "1.6555219\n",
      "Validation Loss Error:  1.525, Batch Validation Time: 27 seconds\n",
      "I speak better now!!\n",
      "1.7096926\n",
      "2.1003737\n",
      "1.3829461\n",
      "1.764174\n",
      "1.7441558\n",
      "1.6412982\n",
      "1.505387\n",
      "1.7320911\n",
      "1.5372546\n",
      "1.6446437\n",
      "1.4669687\n",
      "1.4552621\n",
      "1.6337411\n",
      "1.7333999\n",
      "1.5655189\n",
      "1.5182644\n",
      "1.668861\n",
      "1.5155832\n",
      "1.4274216\n",
      "1.5234232\n",
      "1.5538919\n",
      "Epoch:  10/10, Batch:  100/161, Training Loss Error:  1.594, Training Time on 100 Batches: 252 seconds\n",
      "1.5351802\n",
      "1.2481667\n",
      "1.5976633\n",
      "1.4533279\n",
      "1.6083552\n",
      "1.4502103\n",
      "1.424029\n",
      "1.5591967\n",
      "1.6937551\n",
      "1.8033985\n",
      "1.4749453\n",
      "1.8471795\n",
      "1.4931053\n",
      "1.3945763\n",
      "1.4641428\n",
      "1.8236337\n",
      "1.5738382\n",
      "1.6135719\n",
      "1.6505128\n",
      "1.6761255\n",
      "1.5376912\n",
      "1.6304703\n",
      "1.7925863\n",
      "1.5529196\n",
      "1.3463806\n",
      "1.5154307\n",
      "1.6536232\n",
      "1.7881707\n",
      "1.7158469\n",
      "1.3312223\n",
      "1.5611821\n",
      "1.6595805\n",
      "1.3911659\n",
      "1.6136662\n",
      "1.4670389\n",
      "1.8665459\n",
      "1.5027258\n",
      "1.1955405\n",
      "1.7321739\n",
      "1.3332477\n",
      "1.4299204\n",
      "1.499748\n",
      "1.6071291\n",
      "1.6247265\n",
      "1.675896\n",
      "1.5764874\n",
      "1.6792243\n",
      "1.8223029\n",
      "1.5901333\n",
      "1.4956553\n",
      "1.8012531\n",
      "1.568737\n",
      "1.5181899\n",
      "1.5814121\n",
      "1.4647018\n",
      "1.3007295\n",
      "1.4466815\n",
      "1.7231524\n",
      "Validation Loss Error:  1.517, Batch Validation Time: 27 seconds\n",
      "I speak better now!!\n",
      "1.4398017\n",
      "1.625847\n",
      "Game Over\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 1000\n",
    "checkpoint = path + \"chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        batch_training_loss_errors = session.run([optimizer_gradient_clipping, loss_error],{inputs: padded_questions_in_batch, targets: padded_answers_in_batch, lr: learning_rate, sequence_length: padded_answers_in_batch.shape[1], keep_prob: keep_probability})\n",
    "        batch_training_loss_error = batch_training_loss_errors[1]\n",
    "        print(batch_training_loss_error)\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I speak better now!!')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
    "        break\n",
    "print(\"Game Over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading the weights and Running the session\n",
    "checkpoint = path + \"chatbot_weights.ckpt\"\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the questions from strings to lists of encoding integers\n",
    "def convert_string2int(question, word2int):\n",
    "    question = clean_data(question)\n",
    "    return [word2int.get(word, word2int['<OUT>']) for word in question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hello\n",
      "ChatBot:  I am notout.\n",
      "You: what\n",
      "ChatBot:  I am not aout.\n",
      "You: okay\n",
      "ChatBot:  I know you.\n",
      "You: how do you know me\n",
      "ChatBot:  I am notout.\n",
      "You: okay\n",
      "ChatBot:  I am not aout.\n",
      "You: why\n",
      "ChatBot:  I am notout.\n",
      "You: well gotta go\n",
      "ChatBot:  I am not aout.\n",
      "You: bye bye\n",
      "ChatBot:  I am not aout.\n",
      "You: Goodbye\n"
     ]
    }
   ],
   "source": [
    "# Setting up the chat\n",
    "while(True):\n",
    "    question = input(\"You: \")\n",
    "    if question == 'Goodbye':\n",
    "        break\n",
    "    question = convert_string2int(question, questionswords2int)\n",
    "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
    "    fake_batch = np.zeros((batch_size, 25))\n",
    "    fake_batch[0] = question\n",
    "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
    "    answer = ''\n",
    "    for i in np.argmax(predicted_answer, 1):\n",
    "        if answersints2word[i] == 'i':\n",
    "            token = ' I'\n",
    "        elif answersints2word[i] == '<EOS>':\n",
    "            token = '.'\n",
    "        elif answersints2word[i] == '<OUT>':\n",
    "            token = 'out'\n",
    "        else:\n",
    "            token = ' ' + answersints2word[i]\n",
    "        answer += token\n",
    "        if token == '.':\n",
    "            break\n",
    "    print('ChatBot: ' + answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
